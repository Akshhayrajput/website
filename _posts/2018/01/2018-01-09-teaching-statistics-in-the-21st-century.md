---
layout: post
authors: ["Greg Wilson"]
title: "Teaching Statistics in the 21st Century"
date: 2018-01-09
time: "02:00:00"
category: ["Teaching", "Statistics"]
---

Prof. Daniel Kaplan's 2017 paper
"[Teaching stats for data science](https://peerj.com/preprints/3205/)"
is a great example of how faculty are re-thinking pedagogical approaches
to incorporate ["good enough" practices](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510)
in scientific and statistical computing.
Kaplan argues that much of what we currently teach in introductory stats courses
is left over from a time when data was scarce and calculation was hard.
He advocates a calculation-first approach in ten steps:

1.  Data tables
2.  Data graphics
3.  Model functions
4.  Model training
5.  Effect size and covariates
6.  Displays of distributions
7.  Bootstrap replication
8.  Prediction error
9.  Comparing models
10. Generalization and causality

Many other people are rethinking other subjects along similar lines;
if you have any favorite examples,
please add them to the comments–I'm sure our community would enjoy hearing about them.

> **Abstract**
>
> The familiar mathematical topics of introductory statistics–means,
> proportions, t-tests, normal and t distributions, chi-squared,
> etc.–are a product of the first half of the 20th century. Naturally,
> they reflect the statistical conditions of that era: scarce,
> e.g. n<10, data originating in benchtop or agricultural experiments;
> algorithms communicated via algebraic formulas. Today, applied
> statistics relates to a different environment: software is the means
> of algorithmic communication, observational and "unplanned" data are
> interpreted for causal relationships, and data are large both in n
> and the number of variables. This change in situation calls for a
> thorough rethinking of the topics in and approach to statistics
> education. This paper presents a set of ten organizing blocks for
> intro stats that are better suited to today's environment.
